---
layout: post
title: Using Apache Spark to analyze Seattle Public Library logs
subtitle: Part 2 of Big Data series
image: /img/spark/spark-logo-mod.png
gh-repo: eskilan/sparkWithPython
gh-badge: [star, fork, follow]
---

Analyzing the Seattle Public Library logs again? Well, yes. Last time we used hadoop and mapReduce to find out which items were the most popular in 2017. We also encountered a problem. Figuring out which were the most popular authors would be a little bit convoluted. Is there a better way?
In fact yes. There are many other tools available to perform complex tasks with big data. Some of them build on top of hadoop mapReduce such as Hive. Other tools like Apache Spark also play well with HDFS and YARN but work in a different way.

Spark uses something called Resilient Distributed Datasets or RDDs. An RDD is similar to a file in HDFS except in lives in memory, as in the RAM memory of the cluster. This allows multiple operations to take place without having to write to disk (or disks in HDFS). Also, RDDs dealing with structured data can be treated as dataframes and we are not limited to the mapReduce programming paradigm.

## Installing pySpark
PySpark is the python API library for Spark programming. To install it you can simply type:
```bash
$ pip install pyspark
```

## Spark mapReduce equivalent
In the previous post, we had to write a mapper and a reducer code with several lines of code each. The mapReduce program equivalent is actually really short, actually only one line without the reading and writing to disk.

```python
from pyspark import SparkContext
# cretes SparkContext sc
sc = SparkContext('local','seattleLibraryTest')
# reads file from HDFS at localhost
data = sc.textFile("hdfs://localhost:9000//user/ilan/Checkouts_By_Title_Data_Lens_2017.csv")
# split by commas, map, reduce by key
result = data.map(lambda x: x.split(','))\
    .map(lambda x: (x[0],1))\
    .reduceByKey(lambda x,y: x+y)
# save results
result.saveAsTextFile("hdfs://localhost:9000//user/ilan/sparkResults.csv")
```
The code above reads in data from HDFS. Then, it performs a mapReduce operation within a single line. It splits each line by comma delimiters, then takes the first string and pairs it with a 1, then it does the reduce operation with "reduceByKey." Lastly, it saves the results back to HDFS. It's that simple!

## Finding the most popular authors in 2017
In the following code you will see that I'm using a library called pyspark.sql and DataFrames. With SparkSQL we can use dataframes in a similar way to the way we use Pandas dataframes. With this ability we can substitute mapReduce operations with simple df.groupBy() functions! We can also join multiple data frames with a join() function. The df.toPandas() function will create a copy of the Spark data frame that we can manipulate in the local machine as well. The convenience of this approach cannot be overstated.

```python
# This code performs map reduce on seattle library dataset
from pyspark.sql import SparkSession

sparkSession = SparkSession.builder.appName('seattle-authors').getOrCreate()

# Read from HDFS

df1 = sparkSession.read.csv("hdfs://localhost:9000//user/ilan/Checkouts_By_Title_Data_Lens_2017.csv",header=True)
df2 = sparkSession.read.csv("./Library_Collection_Inventory.csv",header=True) # in local filesystem

# perform map reduce operation on df1 by grouping by BibNumber
countsByBib = df1.groupBy("BibNumber").count()

# create new dataframe RDD from df2 keeping BibNumber, and Author, then removing duplicates and dropping null values
df3 = df2.select(df2['BibNum'],df2['Author']).distinct().dropna()

# joining countsByBib and df3 on countsByBib.bibNumber==df3.BibNum
df4 = countsByBib.join(df3,countsByBib.BibNumber==df3.BibNum,how='inner')

# now we group by author
df5 = df4.select(df4['count'],df4['Author']).groupBy('Author').sum()

# order by descending count
df6 = df5.sort("sum(count)",ascending=False)

# show 
df6.show()

# we can also copy the RDD dataframe to a Pandas dataframe
pDF=df6.toPandas()

sparkSession.stop()
```

The output is the following:
```console
+--------------------+----------+
|              Author|sum(count)|
+--------------------+----------+
|         Willems, Mo|     26645|
|          Seuss, Dr.|     17029|
|   Stilton, Geronimo|     16820|
|Davis, Jim, 1945 ...|     16224|
|      Meadows, Daisy|     14945|
|   Holm, Jennifer L.|     11712|
|  Osborne, Mary Pope|     11071|
|        Arnold, Tedd|      9792|
|Patterson, James,...|      9782|
|  Pilkey, Dav, 1966-|      9593|
|      O'Connor, Jane|      8446|
|     Peirce, Lincoln|      8168|
|    Kusaka, Hidenori|      6599|
|     Rylant, Cynthia|      6515|
|       Riordan, Rick|      5980|
|         Carle, Eric|      5405|
|        Gaiman, Neil|      5357|
|        Hunter, Erin|      5287|
|        Kinney, Jeff|      5269|
|      Rowling, J. K.|      5160|
+--------------------+----------+
only showing top 20 rows
```

The results are quite surprising. A quick online search will reveal that the top authors are writers of children's books! Check it out:

![alt text](/img/spark/MoWillems.png "Mo Willems")
![alt text](/img/spark/DrSeuss.png "Dr. Seuss")
![alt text](/img/spark/GeronimoStilton.png "Geronimo Stilton, actual name is Elisabetta Dami")
![alt text](/img/spark/JimDavis.png "Jim Davis")
![alt text](/img/spark/DaisyMeadows.png "Daisy Meadows")

I believe I've read some books by James Patterson, and I've definitely read all the Harry Potter books by J.K. Rowling =)

## Monitoring with web UI
We can access spark job status by going to port 4040 on the computer that monitors the spark application.
![alt text](/img/spark/webUI.png "Web User Interface for Spark")
It shows that the toPandas() operation failed. That is because I interrupted it manually. After running the command "sparkSession.stop()" the web UI disappears.

I hope you find these Spark examples useful. Remember that the code is in my repository.

Note: The Apache Spark logo By Apache Software Foundation - http://svn.apache.org/repos/asf/hadoop/logos/asf_hadoop/, Apache License 2.0, https://commons.wikimedia.org/w/index.php?curid=63919822 